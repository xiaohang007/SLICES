{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "582b3e45",
   "metadata": {},
   "source": [
    "# Build training sets (skip this part if you have already completed Tutorial_2.1 or 2.2)\n",
    "**!!! We strongly recommend using our Docker image for the setup of Jupyter backend.** \n",
    "**!!! If you choose not to use the Docker image, you'll need to install Slurm on your local machine, which can be tricky.** \n",
    "**!!! If you don't want to install the Slurm workload manager, you'll need to modify the code in utils.py, replacing 'qsub 0_run.pbs' with 'python 0_run.py' inside the splitRun function. Additionally, please ensure that the number of threads does not exceed the number of CPU threads on your computer. Exceeding this limit may lead to resource contention issues.**\n",
    "**!!! 优先使用我们提供的Docker镜像运行Jupyter后端.** \n",
    "**!!! 如果您不想使用Docker镜像，则需要在本机上安装Slurm任务管理系统，这可能会比较复杂。请注意，在运行Jupyter后端之前，需要完成Slurm的安装,否则计算会报错.** \n",
    "**!!! 如果您不想安装Slurm任务管理系统，那么需要修改utils.py的代码，在splitRun函数内部替换 qsub 0_run.pbs为 python 0_run.py，并且确认线程数不会超过电脑的cpu线程数量，否则会出现计算资源挤占的问题.** \n",
    "\n",
    "https://github.com/xiaohang007/SLICES?tab=readme-ov-file#jupyter-backend-setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b72c84",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from slices.utils import temporaryWorkingDirectory,splitRun,show_progress,collect_json,collect_csv,adaptive_dynamic_binning\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with temporaryWorkingDirectory(\"./data/mp20_nonmetal/\"):\n",
    "        output=[]\n",
    "        data_path_predix=\"../mp20/\"\n",
    "        data=pd.read_csv(data_path_predix+\"test.csv\")\n",
    "        cifs=list(data[\"cif\"])\n",
    "        ids=list(data[\"material_id\"])\n",
    "        eform=list(data[\"formation_energy_per_atom\"])\n",
    "        bandgap=list(data[\"band_gap\"])\n",
    "        for i in range(len(ids)):\n",
    "            output.append({\"material_id\":ids[i],\"formation_energy_per_atom\":eform[i],\"cif\":cifs[i],\"band_gap\":bandgap[i]})\n",
    "        data=pd.read_csv(data_path_predix+\"val.csv\")\n",
    "        cifs=list(data[\"cif\"])\n",
    "        ids=list(data[\"material_id\"])\n",
    "        eform=list(data[\"formation_energy_per_atom\"])\n",
    "        bandgap=list(data[\"band_gap\"])\n",
    "        for i in range(len(ids)):\n",
    "            output.append({\"material_id\":ids[i],\"formation_energy_per_atom\":eform[i],\"cif\":cifs[i],\"band_gap\":bandgap[i]})\n",
    "        data=pd.read_csv(data_path_predix+\"train.csv\")\n",
    "        cifs=list(data[\"cif\"])\n",
    "        ids=list(data[\"material_id\"])\n",
    "        eform=list(data[\"formation_energy_per_atom\"])\n",
    "        bandgap=list(data[\"band_gap\"])\n",
    "        for i in range(len(ids)):\n",
    "            output.append({\"material_id\":ids[i],\"formation_energy_per_atom\":eform[i],\"cif\":cifs[i],\"band_gap\":bandgap[i]})\n",
    "        with open('cifs.json', 'w') as f:\n",
    "            json.dump(output, f)\n",
    "        splitRun(filename='./cifs.json',threads=16,skip_header=False)\n",
    "        show_progress()\n",
    "        collect_json(output=\"cifs_filtered.json\", \\\n",
    "            glob_target=\"./**/output.json\",cleanup=False)\n",
    "        collect_csv(output=\"mp20_eform_bandgap_nonmetal.csv\", \\\n",
    "            glob_target=\"./**/result.csv\",cleanup=True,header=\"SLICES,eform,bandgap\\n\")\n",
    "        os.system(\"rm cifs.json\")\n",
    "        # 读取数据\n",
    "        data = pd.read_csv('mp20_eform_bandgap_nonmetal.csv')\n",
    "        target_column = data.columns[-1]  # 假设最后一列是目标值\n",
    "        \n",
    "        # 进行自适应动态分箱\n",
    "        train_data, test_data, bins = adaptive_dynamic_binning(data, target_column)\n",
    "        \n",
    "        # 检查分布\n",
    "        print(\"\\n训练集分布:\")\n",
    "        print(train_data[target_column].value_counts(bins=bins, normalize=True).sort_index())\n",
    "        print(\"\\n测试集分布:\")\n",
    "        print(test_data[target_column].value_counts(bins=bins, normalize=True).sort_index())\n",
    "        \n",
    "        # 保存分割后的数据\n",
    "        train_data.to_csv('train_data_reduce_zero.csv', index=False)\n",
    "        test_data.to_csv('test_data_reduce_zero.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8997fc",
   "metadata": {},
   "source": [
    "# Train MatterGPT for Multi-Property Material Inverse Design (using [bandgap, eform] as an example)\n",
    "<span style=\"color:red\">**CUDA in xiaohang07/slices:chgnet2 Docker works on WSL@Windows 11 but may fail on Ubuntu; for Ubuntu, use host machine to train and generate SLICES with MatterGPT.**</span>\n",
    "\n",
    "**Set --epochs 5 to speed up the test run.**\n",
    "\n",
    "**To accelerate the training process, consider adjusting the batch_size appropriately.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38b5295e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co Co Co Co As As As As S S S S 0 9 ooo 0 6 ooo 0 4 ooo 0 8 ooo 0 11 oo- 0 5 ooo 1 4 ooo 1 10 ooo 1 6 o+o 1 11 ooo 1 7 o+o 1 8 o++ 2 5 --o 2 8 -oo 2 7 -oo 2 10 o-- 2 4 o-o 2 9 ooo 3 7 -oo 3 5 -oo 3 11 -oo 3 6 ooo 3 9 oo+ 3 10 ooo 4 8 o+o 5 9 +oo 6 11 ooo 7 10 +-o \n",
      "[-0.6162958260416668, 0.8586]\n",
      "Constructing vocabulary...\n",
      "Max length of slices:  392\n",
      "Number of characters: 130\n",
      "vocab_size: 132\n",
      "\n",
      "['+++', '++-', '++o', '+-+', '+--', '+-o', '+o+', '+o-', '+oo', '-++', '-+-', '-+o', '--+', '---', '--o', '-o+', '-o-', '-oo', '0', '1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '3', '4', '5', '6', '7', '8', '9', '<', '>', 'Ag', 'Al', 'Ar', 'As', 'Au', 'B', 'Ba', 'Be', 'Bi', 'Br', 'C', 'Ca', 'Cd', 'Ce', 'Cl', 'Co', 'Cr', 'Cs', 'Cu', 'Dy', 'Er', 'Eu', 'F', 'Fe', 'Ga', 'Gd', 'Ge', 'H', 'He', 'Hf', 'Hg', 'Ho', 'I', 'In', 'Ir', 'K', 'Kr', 'La', 'Li', 'Lu', 'Mg', 'Mn', 'Mo', 'N', 'Na', 'Nb', 'Nd', 'Ne', 'Ni', 'O', 'Os', 'P', 'Pb', 'Pd', 'Pm', 'Pr', 'Pt', 'Rb', 'Re', 'Rh', 'Ru', 'S', 'Sb', 'Sc', 'Se', 'Si', 'Sm', 'Sn', 'Sr', 'Ta', 'Tb', 'Tc', 'Te', 'Ti', 'Tl', 'Tm', 'V', 'W', 'Xe', 'Y', 'Yb', 'Zn', 'Zr', 'o++', 'o+-', 'o+o', 'o-+', 'o--', 'o-o', 'oo+', 'oo-', 'ooo']\n",
      "132\n",
      "data has 11720 slices, 132 unique characters.\n",
      "data has 2961 slices, 132 unique characters.\n",
      "The number of trainable parameters is: 85563648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 976: train loss 0.60171. lr 3.032845e-04: 100%|██████████| 977/977 [04:48<00:00,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.4337577740190483 \n",
      "\n",
      "Saving at epoch 1 with best test loss: 0.4337577740190483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2 iter 976: train loss 0.39700. lr 2.219852e-04: 100%|██████████| 977/977 [04:47<00:00,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.37552824658662204 \n",
      "\n",
      "Saving at epoch 2 with best test loss: 0.37552824658662204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 3 iter 976: train loss 0.32866. lr 1.180529e-04: 100%|██████████| 977/977 [04:47<00:00,  3.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.3379483014224512 \n",
      "\n",
      "Saving at epoch 3 with best test loss: 0.3379483014224512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 4 iter 976: train loss 0.23985. lr 3.300000e-05: 100%|██████████| 977/977 [04:47<00:00,  3.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.3136829395405194 \n",
      "\n",
      "Saving at epoch 4 with best test loss: 0.3136829395405194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 5 iter 976: train loss 0.33603. lr 3.300000e-05: 100%|██████████| 977/977 [04:47<00:00,  3.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.3058865898051243 \n",
      "\n",
      "Saving at epoch 5 with best test loss: 0.3058865898051243\n"
     ]
    }
   ],
   "source": [
    "from slices.utils import temporaryWorkingDirectory\n",
    "import os\n",
    "with temporaryWorkingDirectory(\"./MatterGPT/bandgap_eform/1_train_generate\"):\n",
    "    os.system('''\n",
    "    /bin/bash -c \"source /opt/conda/etc/profile.d/conda.sh && conda activate chgnet && \\\n",
    "    python train.py --run_name bandgap_Aug1 --batch_size 12 --num_props 2 --max_epochs 5 --n_embd 768  --n_layer 12 --n_head 12 --learning_rate 3.3e-4 \\\n",
    "    --train_dataset '../../../data/mp20_nonmetal/train_data_reduce_zero.csv' --test_dataset '../../../data/mp20_nonmetal/test_data_reduce_zero.csv' \"\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ff1c75",
   "metadata": {},
   "source": [
    "# Generate SLICES strings with specified [$E_{form}$, $E_{gap}$] = [-2.0 eV/atom, 1.0 eV]  \n",
    "<span style=\"color:red\">**CUDA in xiaohang07/slices:chgnet2 Docker works on WSL@Windows 11 but may fail on Ubuntu; for Ubuntu, use host machine to train and generate SLICES with MatterGPT.**</span>\n",
    "\n",
    "**Set --gen_size 20 to speed up the test run.**\n",
    "\n",
    "**To accelerate the generation process, consider adjusting the batch_size appropriately.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44c5226a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target 1 is bandgap: 1.0 eV and eform: -2.0 eV/atom.\n",
      "./Voc_prior\n",
      "Loading model\n",
      "Model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:06<00:00,  6.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total generated SLICES: 19\n",
      "Unique canonical SLICES: 19\n",
      "Valid ratio: 0.95\n",
      "Unique ratio: 1.0\n"
     ]
    }
   ],
   "source": [
    "from slices.utils import temporaryWorkingDirectory\n",
    "import os\n",
    "with temporaryWorkingDirectory(\"./MatterGPT/bandgap_eform/1_train_generate\"):\n",
    "    os.system('''\n",
    "    /bin/bash -c \"source /opt/conda/etc/profile.d/conda.sh && conda activate chgnet && \\\n",
    "    python generate.py --model_weight bandgap_Aug1.pt --prop_targets '[[-2.0,1.0]]' --gen_size 20 --batch_size 2 --csv_name inverse_designed_2props_SLICES  --n_head 12\"\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e730a8ee",
   "metadata": {},
   "source": [
    "# Reconstruct crystals from SLICES and assess novelty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89d9fd23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computational tasks have been submitted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     100%|███████████████| 100/100 [15:12<00:00,  9.13s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results have been collected into: results.csv\n"
     ]
    }
   ],
   "source": [
    "from slices.utils import temporaryWorkingDirectory,splitRun_csv,show_progress,collect_csv\n",
    "import os\n",
    "import glob\n",
    "from slices.utils import splitRun_csv, show_progress, collect_csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "import matplotlib.ticker as ticker\n",
    "import pickle,json\n",
    "from pymatgen.core.structure import Structure\n",
    "def load_and_save_structure_database():\n",
    "    with open('../../../data/mp20_nonmetal/cifs_filtered.json', 'r') as f:\n",
    "        cifs = json.load(f)\n",
    "    \n",
    "    structure_database = []\n",
    "    for i in range(len(cifs)):\n",
    "        cif_string = cifs[i][\"cif\"]\n",
    "        try:\n",
    "            stru = Structure.from_str(cif_string, \"cif\")\n",
    "            structure_database.append([stru, cifs[i][\"band_gap\"]])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    # Serialize the data\n",
    "    with open('structure_database.pkl', 'wb') as f:\n",
    "        pickle.dump(structure_database, f)\n",
    "        \n",
    "def process_data():\n",
    "    load_and_save_structure_database()\n",
    "    splitRun_csv(filename='../1_train_generate/inverse_designed_2props_SLICES.csv', threads=8, skip_header=True)\n",
    "    show_progress()\n",
    "    collect_csv(output=\"results.csv\",\n",
    "                glob_target=\"./job_*/result.csv\", cleanup=True,\n",
    "                header=\"bandgap_target,eform_target,SLICES,poscar,novelty\\n\")\n",
    "if __name__ == \"__main__\":\n",
    "    with temporaryWorkingDirectory(\"./MatterGPT/bandgap_eform/2_inverse_novelty\"):\n",
    "        process_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3408073a",
   "metadata": {},
   "source": [
    "# Evaluate the bandgap & eform distribution of the reconstructed crystals at PBE level (need workstation or even HPC to run VASP fastly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7a6878-c05d-4d70-8e4b-30d920f6446c",
   "metadata": {},
   "outputs": [],
   "source": [
    "```bash\n",
    "cd ./MatterGPT/bandgap_eform/3_DFT\n",
    "python 1_run.py\n",
    "# done\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
